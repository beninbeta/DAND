{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze A/B Test Results for New Web Page vs Old Webpage\n",
    "\n",
    "\n",
    "## Table of Contents\n",
    "- [Introduction](#intro)\n",
    "- [Part I - Probability](#probability)\n",
    "- [Part II - A/B Test](#ab_test)\n",
    "- [Part III - Regression](#regression)\n",
    "\n",
    "\n",
    "<a id='intro'></a>\n",
    "### Introduction\n",
    "\n",
    "This notebook was created to Analyze obtained from client on an A/B test they conducted to see which if their new web page has helped to convert more users that their old web page. \n",
    "\n",
    "In this analysis we will be using the python programming language along with the pandas, numpy, matplotlib.pyplot, and statsmodel librarys to perform the analysis and provide a recommendation on which web page appeared to perform the best based on the data provided.\n",
    "\n",
    "#### Analysis Note:\n",
    "Because the dataset provided could have imperfections, incommplete or missing data, these recommendations only apply to the data provided.\n",
    "\n",
    "<a id='probability'></a>\n",
    "#### Part I - Probability\n",
    "\n",
    "To get started, we imported several of the libraries needed for this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#We are setting the seed to assure you get the same answers on quizzes as we set up\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`1.` We read in the `ab_data.csv` data and stored it in `df`.  \n",
    "\n",
    "a. Read in the dataset and looked at the top few rows as well as looked at the spread in the timestamp data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2017-01-02 13:42:05.378582',\n",
       " '2017-01-24 13:41:54.460509',\n",
       "    user_id                   timestamp      group landing_page  converted\n",
       " 0   851104  2017-01-21 22:11:48.556739    control     old_page          0\n",
       " 1   804228  2017-01-12 08:01:45.159739    control     old_page          0\n",
       " 2   661590  2017-01-11 16:55:06.154213  treatment     new_page          0\n",
       " 3   853541  2017-01-08 18:28:03.143765  treatment     new_page          0\n",
       " 4   864975  2017-01-21 01:52:26.210827    control     old_page          1)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read in the ab dataset and do a quick check\n",
    "#we found that this data was for about 22 days in Jan of 2017\n",
    "df = pd.read_csv('ab_data.csv')\n",
    "df.head()\n",
    "df.timestamp.min(), df.timestamp.max(), df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. We found the number of rows in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(294478, 5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checked to see how big the data set is\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. The number of unique users in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "290584"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#since we only want unique users, we wanted to see how many there are\n",
    "df['user_id'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "d. The proportion of users converted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11965919355605512"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#since our investigation is at which page is better we \n",
    "#look to see how many converted overall\n",
    "overall_converted = df.query('converted == 1')['user_id'].count()/df.shape[0]\n",
    "overall_converted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "e. The number of times the `new_page` and `treatment` don't match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3893"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#next we looked at the data for our groups and pages\n",
    "#to see if there were any descrepencies\n",
    "df.query('landing_page == \"new_page\" and group != \"treatment\"')['user_id'].count() + df.query(\"group == 'treatment' and landing_page != 'new_page'\")['user_id'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f. We looked for any missing values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 294478 entries, 0 to 294477\n",
      "Data columns (total 5 columns):\n",
      "user_id         294478 non-null int64\n",
      "timestamp       294478 non-null object\n",
      "group           294478 non-null object\n",
      "landing_page    294478 non-null object\n",
      "converted       294478 non-null int64\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 11.2+ MB\n"
     ]
    }
   ],
   "source": [
    "#we also looked to see if there was any missing data\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.` \n",
    "## Data Cleaning\n",
    "\n",
    "a. Since we found that there was not any missing data, and that there were some descrepencies in the data we created a new Dataframe df2 with only the correct values we were looking for where landing_pagea and control matched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "290585"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#next we created a dataframe from the orginal data, with just \n",
    "#the correct data for groups and pages (data cleaning)\n",
    "df2 = df.query('(landing_page == \"new_page\" and group == \"treatment\") or (group == \"control\" and landing_page == \"old_page\")')\n",
    "\n",
    "df2.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Double Checked all of the correct rows were removed - this should be 0\n",
    "df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3.` Data Cleaning Cont..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Numer of unique **user_id**s in **df2**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id         290584\n",
       "timestamp       290585\n",
       "group                2\n",
       "landing_page         2\n",
       "converted            2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#next we looked for any dupclicates\n",
    "df2.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "b. We found one user was duplicated and the removed one entry for that user keep the first one only.\n",
    "c. Row for the duplicated user was 2893 and 1899"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2893</th>\n",
       "      <td>773192</td>\n",
       "      <td>2017-01-14 02:55:59.590927</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id                   timestamp      group landing_page  converted\n",
       "2893   773192  2017-01-14 02:55:59.590927  treatment     new_page          0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we found one duplicate user\n",
    "df2[df2.duplicated('user_id')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1899</th>\n",
       "      <td>773192</td>\n",
       "      <td>2017-01-09 05:37:58.781806</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2893</th>\n",
       "      <td>773192</td>\n",
       "      <td>2017-01-14 02:55:59.590927</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id                   timestamp      group landing_page  converted\n",
       "1899   773192  2017-01-09 05:37:58.781806  treatment     new_page          0\n",
       "2893   773192  2017-01-14 02:55:59.590927  treatment     new_page          0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#looked at both duplicates\n",
    "df2.loc[df2['user_id'] == 773192]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. Removed **one** of the rows with a duplicate **user_id**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1899</th>\n",
       "      <td>773192</td>\n",
       "      <td>2017-01-09 05:37:58.781806</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id                   timestamp      group landing_page  converted\n",
       "1899   773192  2017-01-09 05:37:58.781806  treatment     new_page          0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#kept the first enrty. Since they were the same besides time\n",
    "df2 = df2.drop_duplicates(subset='user_id', keep='first')\n",
    "df2.loc[df2['user_id'] == 773192]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`4.` **Data Investigaion**\n",
    "\n",
    "a. Since we were interested in the conversion for these two pages, we first looked at data provided on conversion rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11959708724499628"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the probability that anyone might convert reguardless of the page\n",
    "df2['converted'].mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Given that an individual was in the `control` group, the probability they converted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1203863045004612"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#probability of the control converting reguradless of page\n",
    "p_control = df2.query('group == \"control\"')['converted'].mean()\n",
    "p_control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Given that an individual was in the `treatment` group, the probability they converted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11880806551510564"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#probability of the treatment group converting requardless of page\n",
    "p_treatment = df2.query('group == \"treatment\"')['converted'].mean()\n",
    "p_treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0015782389853555567"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the observed difference of the new page and the old page or p_new-p_old\n",
    "#we will use this to determine if this data fits into our null or alternate hypothesis\n",
    "obs_diff = p_treatment - p_control\n",
    "obs_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. The probability that an individual received the new page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.50006194422266881"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The probablity that person received a new page regaurdles of the group\n",
    "df2.query('landing_page == \"new_page\"').count()['converted']/df2.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. Below are some intial thoughts on what was found in Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initial look at probability**\n",
    "\n",
    "Given what we found above it would appear that there is evidence to support that the old page is perfroming better than the new page. However, we need to invetigate further and look at the length of this comparison and do some a/b test simulations as well to get more information before making any conclusions. \n",
    "\n",
    "We will also use a z-test to check those numbers and make sure we have done the distributions correctly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ab_test'></a>\n",
    "### Part II - A/B Test\n",
    "\n",
    "\n",
    "Because of the time stamp associated with each event, you could technically run a hypothesis test continuously as each observation was observed.  However it is difficult to know if 22 days is long enougth to get through user bias around change. For this part of the anaysis we will concentrate on the covnversion rate of the new vs web page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`1.`\n",
    "**We will be testing to see if the new page is better than that old, we are assuming that the old page is better unless the new page proves better by at least a Type I error rate of 5%**\n",
    "\n",
    "$$H_0: p_{new} - p_{old} \\leq 0$$\n",
    "\n",
    "\n",
    "$$H_1: p_{new} - p_{old} > 0$$\n",
    "\n",
    "\n",
    "**$p_{old}$ and $p_{new}$ are the mean values of conversions for the Old page and the new page, respectivley.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.` We will ssume under the null hypothesis, $p_{new}$ and $p_{old}$ both have \"true\" success rates equal to the **converted** success rate regardless of page - that is $p_{new}$ and $p_{old}$ are equal. Furthermore, we assumed they are equal to the **converted** rate in **ab_data.csv** regardless of the page. <br><br>\n",
    "\n",
    "We used a sample size for each page equal to the ones in **ab_data.csv**.  <br><br>\n",
    "\n",
    "Below, we performed the sampling distribution for the difference in **converted** between the two pages over 10,000 iterations of calculating an estimate from the null.  <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. The **conversion rate** for $p_{new}$ under the null. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11959708724499628"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#found the conversion rate for the new page which we determined to be the the overall success rate\n",
    "#under the null\n",
    "p_new = (df2.converted).mean()\n",
    "p_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. The **conversion rate** for $p_{old}$ under the null. <br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11959708724499628"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#found the conversion rate for the old page which we determined to be the the overall success rate\n",
    "#under the null\n",
    "p_old = (df2.converted).mean()\n",
    "p_old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. $n_{new}$, is the number of individuals in the treatment group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145310"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the number of those getting the new page\n",
    "n_new = df2.query('landing_page == \"new_page\"').count()[\"user_id\"]\n",
    "n_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. $n_{old}$, is the number of individuals in the control group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145274"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the number of those geting the old page\n",
    "n_old = df2.query('landing_page == \"old_page\"').count()[\"user_id\"]\n",
    "n_old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. Simulation of $n_{new}$ transactions with a conversion rate of $p_{new}$ under the null.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11948248572018444"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#simulate using number of new transactions with the conversion rate of the new page\n",
    "new_page_converted = np.random.choice([0,1], n_new, p=(1-p_new,p_new ))\n",
    "#the new page simulated conversion rate\n",
    "new_page_converted.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f. Simulation of $n_{old}$ transactions with a conversion rate of $p_{old}$ under the null.  S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12064099563583298"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#simulate using the number of people getting the old page using the old page conversion rate\n",
    "old_page_converted = np.random.choice([0,1], n_old, p=(1-p_old, p_old))\n",
    "#the old page simulated conversion rate\n",
    "old_page_converted.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g. Found $p_{new}$ - $p_{old}$ for the simulated values from part (e) and (f)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0011585099156485451"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the difference in the conversion rates which we expect to be zero\n",
    "new_page_converted.mean() - old_page_converted.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h. Created a bootstrap sample of 10,000 $p_{new}$ - $p_{old}$ values using the same simulation process in parts (a) through (g) above. This allows us to run 10,000 tests and the law of large numbers to simulate a much larger sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.8496274170259134e-06"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#next we bootstrap samples \n",
    "\n",
    "p_diffs = []\n",
    "for n in range(10000):\n",
    "    np_con = np.random.choice([0,1], n_new, p=(1-p_new,p_new ))\n",
    "    op_con = np.random.choice([0,1], n_old, p=(1-p_old, p_old))\n",
    "    diff = np_con.mean() - op_con.mean()\n",
    "    p_diffs.append(diff)\n",
    "np.array(p_diffs).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i. A histogram of the bootstrap samples **p_diffs**.  We also added the observed values from the orginal dataset found in 1(c)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb7c4d0fda0>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot a historgram with our observed difference to see if it falls within the null samples\n",
    "std_pdiffs = np.std(p_diffs)\n",
    "plt.hist(p_diffs);\n",
    "plt.axvline(x=obs_diff, color=\"red\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "j. The proportion of the our bootstap samples that are greater than the actual difference observed in **ab_data.csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90749999999999997"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We need to know the proportion of of test that fell above the observed difference\n",
    "#in the acutal data\n",
    "(p_diffs > obs_diff).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k. Please explain using the vocabulary you've learned in this course what you just computed in part **j.**  What is this value called in scientific studies?  What does this value mean in terms of whether or not there is a difference between the new and old pages?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sampling preliminary conclusions**\n",
    "\n",
    "After creating samples and using bootstraping, our pvalue was .908 and far above the Type I error rate of 0.05. Therefore it's a strong indicator that we should **not reject the null** in this case and likely stay with the old page. \n",
    "\n",
    "However, to ensure that our tests were done properly, we need another method to check this value against. Next, we completed a ztest using the statsmodel ztest function. If we get a similiar result to the above, we have further evidence that we should not reject the null and that we did our calculation correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "l. Below we calculated the number of conversions for each page, as well as the number of individuals who received each page. `n_old` and `n_new` refer the the number of rows associated with the old page and new pages, respectively. First, we imported the statsmodels library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "#imported the statsmodel library\n",
    "import statsmodels.api as sm\n",
    "#calculated the totals of the converted control and treatment groups\n",
    "convert_old = sum((df2.group == 'control') & (df2.converted == 1)) \n",
    "convert_new = sum((df2.group == 'treatment') & (df2.converted == 1)) \n",
    "#found the total number of each page\n",
    "n_old = (df2.landing_page == 'old_page').value_counts()[True]\n",
    "n_new = (df2.landing_page == 'new_page').value_counts()[True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "m. We used `stats.proportions_ztest` to compute your test statistic and p-value.  [Here](http://knowledgetack.com/python/statsmodels/proportions_ztest/) is a helpful link if you want to learn more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.3109241984234394, 0.90505831275902449)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#z_score is the std deviation and p_value is the pvalue of this data calculated by \n",
    "#the statsmodel ztest function\n",
    "z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old], alternative='larger') \n",
    "z_score, p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n. Below are the findings from the z_test and how they compared to the bootstrapped sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Z-Test Results**\n",
    "\n",
    "After completing the z-test, we do have aggreement with our distibution sampling that our pvaule is about .91. This indicates that given all other data being equal, we should **not reject the null** in this case and should instead recommend using the old_page.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='regression'></a>\n",
    "### Part III - A regression approach\n",
    "\n",
    "`1.`Lastly to further confirm our findings and to look at one other metric, location, we used logitic regression on this data set as well. \n",
    "\n",
    "a. The reasoning for this approach is below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regression Approach**\n",
    "\n",
    "Because we have a row that is either conversion or no conversion, we are going to use logistic regression for this case as the result is binomial. We will start by splitting the conversion into to columns, we will then drop one of them in order to compare them acurately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. First, we added an **intercept** column, as well as an **ab_page** column, which is 1 when an individual receives the **treatment** and 0 if **control**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "      <th>intercept</th>\n",
       "      <th>ab_page</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>851104</td>\n",
       "      <td>2017-01-21 22:11:48.556739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>804228</td>\n",
       "      <td>2017-01-12 08:01:45.159739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>661590</td>\n",
       "      <td>2017-01-11 16:55:06.154213</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>853541</td>\n",
       "      <td>2017-01-08 18:28:03.143765</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>864975</td>\n",
       "      <td>2017-01-21 01:52:26.210827</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                   timestamp      group landing_page  converted  \\\n",
       "0   851104  2017-01-21 22:11:48.556739    control     old_page          0   \n",
       "1   804228  2017-01-12 08:01:45.159739    control     old_page          0   \n",
       "2   661590  2017-01-11 16:55:06.154213  treatment     new_page          0   \n",
       "3   853541  2017-01-08 18:28:03.143765  treatment     new_page          0   \n",
       "4   864975  2017-01-21 01:52:26.210827    control     old_page          1   \n",
       "\n",
       "   intercept  ab_page  \n",
       "0          1        0  \n",
       "1          1        0  \n",
       "2          1        1  \n",
       "3          1        1  \n",
       "4          1        0  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the model needs an intercept so we provide that.\n",
    "df2['intercept'] = 1\n",
    "df2[['group1','ab_page']] = pd.get_dummies(df2['group'])\n",
    "df2 = df2.drop('group1', axis=1)\n",
    "#Aquick look at the new dataframe after adding these columns\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we used statsmodels logistic regression function to look at our a/b comparision from above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.366118\n",
      "         Iterations 6\n"
     ]
    }
   ],
   "source": [
    "#since converted is what we want to predict it is the y value, while ab_page is the predictor in this case\n",
    "y= df2['converted']\n",
    "x = df2[['intercept', 'ab_page']]\n",
    "log_mod = sm.Logit(y,x)\n",
    "results = log_mod.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. Summary of the model above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>     <td>converted</td>    <th>  No. Observations:  </th>   <td>290584</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>               <td>Logit</td>      <th>  Df Residuals:      </th>   <td>290582</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>               <td>MLE</td>       <th>  Df Model:          </th>   <td>     1</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>          <td>Tue, 14 May 2019</td> <th>  Pseudo R-squ.:     </th>  <td>8.077e-06</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>              <td>01:56:11</td>     <th>  Log-Likelihood:    </th> <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>           <td>True</td>       <th>  LL-Null:           </th> <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th> </th>                      <td> </td>        <th>  LLR p-value:       </th>   <td>0.1899</td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td>   -1.9888</td> <td>    0.008</td> <td> -246.669</td> <td> 0.000</td> <td>   -2.005</td> <td>   -1.973</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ab_page</th>   <td>   -0.0150</td> <td>    0.011</td> <td>   -1.311</td> <td> 0.190</td> <td>   -0.037</td> <td>    0.007</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:              converted   No. Observations:               290584\n",
       "Model:                          Logit   Df Residuals:                   290582\n",
       "Method:                           MLE   Df Model:                            1\n",
       "Date:                Tue, 14 May 2019   Pseudo R-squ.:               8.077e-06\n",
       "Time:                        01:56:11   Log-Likelihood:            -1.0639e+05\n",
       "converged:                       True   LL-Null:                   -1.0639e+05\n",
       "                                        LLR p-value:                    0.1899\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "intercept     -1.9888      0.008   -246.669      0.000      -2.005      -1.973\n",
       "ab_page       -0.0150      0.011     -1.311      0.190      -0.037       0.007\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. Below are the results from the Logigistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logit Results**\n",
    "\n",
    "In this test, we found the pvalue to be .190 which is different than the tests we did in part 2. However, logit is a two tailed test. Therefore our null is \n",
    "$$H_0: p_{old} = 0$$\n",
    "\n",
    "\n",
    "$$H_1: p_{new} \\neq 0$$\n",
    "\n",
    "where the former was the \n",
    "\n",
    "$$H_0: p_{new} - p_{old} \\leq 0$$\n",
    "\n",
    "\n",
    "$$H_1: p_{new} - p_{old} > 0$$\n",
    "\n",
    "Given that infomration we have found that this test further confirms what we did in part 2. \n",
    "\n",
    "All other things being equal we should **not reject the null** becuase the pvalue .190 is greater than the 0.05 error rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f. Other factors we may or may not consider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Other considerations**\n",
    "\n",
    "While we did find evidence to not reject the null in all cases, it might be good to investigate some other factors like location or even time. However, with time it might be difficult to know how long to conduct the testing for and our data, may not have enough samples over a significant period. Coninuing the study for an extended period might be good, but we must work with the data we already have.\n",
    "\n",
    "Instead looking at the country along with the conversion rate would give use some potenial insights quickly without the variable of time allowing us to determine if certain areas should use the new page in certain countries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g. Now along with testing if the conversion rate changes for different pages, we also added an effect based on which country a user lives in. We first read in the **countries.csv** dataset and merged together with df2.  i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we joined the country data with the dataframe based on user_id\n",
    "country_data = pd.read_csv('countries.csv')\n",
    "df2 = df2.join(country_data, lsuffix='user_id', rsuffix='user_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "      <th>intercept</th>\n",
       "      <th>ab_page</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-21 22:11:48.556739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-12 08:01:45.159739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-11 16:55:06.154213</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-08 18:28:03.143765</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-21 01:52:26.210827</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    timestamp      group landing_page  converted  intercept  \\\n",
       "0  2017-01-21 22:11:48.556739    control     old_page          0          1   \n",
       "1  2017-01-12 08:01:45.159739    control     old_page          0          1   \n",
       "2  2017-01-11 16:55:06.154213  treatment     new_page          0          1   \n",
       "3  2017-01-08 18:28:03.143765  treatment     new_page          0          1   \n",
       "4  2017-01-21 01:52:26.210827    control     old_page          1          1   \n",
       "\n",
       "   ab_page country  \n",
       "0        0      UK  \n",
       "1        0      US  \n",
       "2        1      UK  \n",
       "3        1      UK  \n",
       "4        0      UK  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we dropped the extra userid field\n",
    "df2 = df2.drop(df2['user_iduser_id'], axis=1)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.00116621485786 -0.00241283252808 0.00629272518173\n"
     ]
    }
   ],
   "source": [
    "#This is a quick examimation of the data to see if we think there might be some significance of the location\n",
    "uk_mean = df2.query(\"country == 'UK' and group == 'treatment'\")['converted'].mean() - df2.query(\"country == 'UK' and group == 'control'\")['converted'].mean()\n",
    "us_mean = df2.query(\"country == 'US' and group == 'treatment'\")['converted'].mean() - df2.query(\"country == 'US' and group == 'control'\")['converted'].mean() \n",
    "ca_mean = df2.query(\"country == 'CA' and group == 'treatment'\")['converted'].mean() - df2.query(\"country == 'CA' and group == 'control'\")['converted'].mean()\n",
    "print(uk_mean, us_mean,ca_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quick Look at Country Data**\n",
    "In a quick investigation of the country data, it looks like there may be some evidence that the new_page is performing better in Canada and the US. Below we will invesigate further to find out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "      <th>intercept</th>\n",
       "      <th>ab_page</th>\n",
       "      <th>country</th>\n",
       "      <th>uk</th>\n",
       "      <th>us</th>\n",
       "      <th>ca</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-21 22:11:48.556739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>UK</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-12 08:01:45.159739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-11 16:55:06.154213</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>UK</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-08 18:28:03.143765</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>UK</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-21 01:52:26.210827</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>UK</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    timestamp      group landing_page  converted  intercept  \\\n",
       "0  2017-01-21 22:11:48.556739    control     old_page          0          1   \n",
       "1  2017-01-12 08:01:45.159739    control     old_page          0          1   \n",
       "2  2017-01-11 16:55:06.154213  treatment     new_page          0          1   \n",
       "3  2017-01-08 18:28:03.143765  treatment     new_page          0          1   \n",
       "4  2017-01-21 01:52:26.210827    control     old_page          1          1   \n",
       "\n",
       "   ab_page country  uk  us  ca  \n",
       "0        0      UK   0   1   0  \n",
       "1        0      US   0   0   1  \n",
       "2        1      UK   0   1   0  \n",
       "3        1      UK   0   1   0  \n",
       "4        0      UK   0   1   0  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Here we created separte columns for the countries\n",
    "df2[['uk','us','ca']] = pd.get_dummies(df2['country'])\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "      <th>intercept</th>\n",
       "      <th>ab_page</th>\n",
       "      <th>country</th>\n",
       "      <th>us</th>\n",
       "      <th>ca</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-21 22:11:48.556739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>UK</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-12 08:01:45.159739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-11 16:55:06.154213</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>UK</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-08 18:28:03.143765</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>UK</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-21 01:52:26.210827</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>UK</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    timestamp      group landing_page  converted  intercept  \\\n",
       "0  2017-01-21 22:11:48.556739    control     old_page          0          1   \n",
       "1  2017-01-12 08:01:45.159739    control     old_page          0          1   \n",
       "2  2017-01-11 16:55:06.154213  treatment     new_page          0          1   \n",
       "3  2017-01-08 18:28:03.143765  treatment     new_page          0          1   \n",
       "4  2017-01-21 01:52:26.210827    control     old_page          1          1   \n",
       "\n",
       "   ab_page country  us  ca  \n",
       "0        0      UK   1   0  \n",
       "1        0      US   0   1  \n",
       "2        1      UK   1   0  \n",
       "3        1      UK   1   0  \n",
       "4        0      UK   1   0  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we then dropped off uk for the logistic regression\n",
    "df2 = df2.drop('uk', axis=1)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "      <th>intercept</th>\n",
       "      <th>ab_page</th>\n",
       "      <th>country</th>\n",
       "      <th>us</th>\n",
       "      <th>ca</th>\n",
       "      <th>ca_page</th>\n",
       "      <th>us_page</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-21 22:11:48.556739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>UK</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-12 08:01:45.159739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-11 16:55:06.154213</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>UK</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-08 18:28:03.143765</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>UK</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-21 01:52:26.210827</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>UK</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    timestamp      group landing_page  converted  intercept  \\\n",
       "0  2017-01-21 22:11:48.556739    control     old_page          0          1   \n",
       "1  2017-01-12 08:01:45.159739    control     old_page          0          1   \n",
       "2  2017-01-11 16:55:06.154213  treatment     new_page          0          1   \n",
       "3  2017-01-08 18:28:03.143765  treatment     new_page          0          1   \n",
       "4  2017-01-21 01:52:26.210827    control     old_page          1          1   \n",
       "\n",
       "   ab_page country  us  ca  ca_page  us_page  \n",
       "0        0      UK   1   0        0        0  \n",
       "1        0      US   0   1        0        0  \n",
       "2        1      UK   1   0        0        1  \n",
       "3        1      UK   1   0        0        1  \n",
       "4        0      UK   1   0        0        0  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we then created a column with a score for each row based on the counrtry and page it received\n",
    "df2['ca_page']= df2['ab_page']*df2['ca']\n",
    "df2['us_page']= df2['ab_page']*df2['us']\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h. Next we completed the logistic regression based on the location as well.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.366117\n",
      "         Iterations 6\n"
     ]
    }
   ],
   "source": [
    "y = df2['converted']\n",
    "x = df2[['intercept', 'ab_page','ca_page','us_page']]\n",
    "cmod = sm.Logit(y,x)\n",
    "res = cmod.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>     <td>converted</td>    <th>  No. Observations:  </th>   <td>290584</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>               <td>Logit</td>      <th>  Df Residuals:      </th>   <td>290580</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>               <td>MLE</td>       <th>  Df Model:          </th>   <td>     3</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>          <td>Tue, 14 May 2019</td> <th>  Pseudo R-squ.:     </th>  <td>1.082e-05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>              <td>01:56:29</td>     <th>  Log-Likelihood:    </th> <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>           <td>True</td>       <th>  LL-Null:           </th> <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th> </th>                      <td> </td>        <th>  LLR p-value:       </th>   <td>0.5119</td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td>   -1.9888</td> <td>    0.008</td> <td> -246.669</td> <td> 0.000</td> <td>   -2.005</td> <td>   -1.973</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ab_page</th>   <td>   -0.0020</td> <td>    0.033</td> <td>   -0.060</td> <td> 0.952</td> <td>   -0.067</td> <td>    0.063</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ca_page</th>   <td>   -0.0171</td> <td>    0.034</td> <td>   -0.507</td> <td> 0.612</td> <td>   -0.083</td> <td>    0.049</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>us_page</th>   <td>   -0.0049</td> <td>    0.036</td> <td>   -0.134</td> <td> 0.893</td> <td>   -0.076</td> <td>    0.066</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:              converted   No. Observations:               290584\n",
       "Model:                          Logit   Df Residuals:                   290580\n",
       "Method:                           MLE   Df Model:                            3\n",
       "Date:                Tue, 14 May 2019   Pseudo R-squ.:               1.082e-05\n",
       "Time:                        01:56:29   Log-Likelihood:            -1.0639e+05\n",
       "converged:                       True   LL-Null:                   -1.0639e+05\n",
       "                                        LLR p-value:                    0.5119\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "intercept     -1.9888      0.008   -246.669      0.000      -2.005      -1.973\n",
       "ab_page       -0.0020      0.033     -0.060      0.952      -0.067       0.063\n",
       "ca_page       -0.0171      0.034     -0.507      0.612      -0.083       0.049\n",
       "us_page       -0.0049      0.036     -0.134      0.893      -0.076       0.066\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regression of Contry Data Summary**\n",
    "Even when broken up, the analysis suggests what we should not reject the null hypothesis. None of the values were statistically significant for any of the countries. \n",
    "\n",
    "There is some evidence that the new page performs slightly better in Canda in particular, but it is not statiscially significant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusions**\n",
    "**Summary Finding Warning**\n",
    "It needs to be stated that these conclusions are based only on the data provided and therefore could be incomplete or incorrect.\n",
    "\n",
    "##Findings and Recommendations\n",
    "After investigating this data set in multiple ways, we would recommend **not rejecting the null hypothesis** and therefore continuing to use the old page. \n",
    "\n",
    "Looking back at the analysis we looked at the following hypothesis:\n",
    "Therefore our null is \n",
    "\n",
    "$$H_0: p_{new} - p_{old} \\leq 0$$\n",
    "\n",
    "\n",
    "$$H_1: p_{new} - p_{old} > 0$$\n",
    "\n",
    "In the first explorations, we our data supported not rejecting the null with a .910 pvalue which is far above the accecpted error rate of 0.05.\n",
    "\n",
    "Our actual data from Part I and sample data, bootstrap sample, and z-test from Part II all showed very similar results. Further our Logistic Regression analysis in Part III further supported that we should no reject the null hypothesis. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Directions to Submit\n",
    "\n",
    "> Before you submit your project, you need to create a .html or .pdf version of this notebook in the workspace here. To do that, run the code cell below. If it worked correctly, you should get a return code of 0, and you should see the generated .html file in the workspace directory (click on the orange Jupyter icon in the upper left).\n",
    "\n",
    "> Alternatively, you can download this report as .html via the **File** > **Download as** submenu, and then manually upload it into the workspace directory by clicking on the orange Jupyter icon in the upper left, then using the Upload button.\n",
    "\n",
    "> Once you've done this, you can submit your project by clicking on the \"Submit Project\" button in the lower right here. This will create and submit a zip file with this .ipynb doc and the .html or .pdf version you created. Congratulations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from subprocess import call\n",
    "call(['python', '-m', 'nbconvert', 'Analyze_ab_test_results_notebook.ipynb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
